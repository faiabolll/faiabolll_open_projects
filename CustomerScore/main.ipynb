{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from ipypb import track\n",
    "from glob import glob\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bq-results-20211205-233906-s3fcmqg6kwal.csv')\n",
    "df = df[['anonymousId', 'isTransaction', 'totalTransactions_windowed', 'eventNumBeforeTransaction', 'totalTransactionPerUser']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10000\n",
    "n_steps=51\n",
    "freq1, freq2, offsets1, offsets2 = np.random.rand(4,batch_size,1)\n",
    "time=np.linspace(0,1,n_steps)\n",
    "series = 0.5*np.sin((time-offsets1)*(freq1*10+10))\n",
    "seriesn = series[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = seriesn[:7000,:n_steps-1], seriesn[:7000,-1]\n",
    "X_valid, y_valid = seriesn[7000:9000,:n_steps-1], seriesn[7000:9000,-1]\n",
    "X_test, y_test = seriesn[9000:,:n_steps-1], seriesn[9000:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50,1]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_lin.compile(loss='mse', optimizer='adam')\n",
    "history = model_lin.fit(X_train, y_train, verbose=0, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential(\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None,1])\n",
    ")\n",
    "model_rnn.compile(loss='mse', optimizer='adam')\n",
    "history = model_lin.fit(X_train, y_train, verbose=0, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка на пробном рабочем датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df['anonymousId'].unique()\n",
    "for user in users:\n",
    "    df[df['anonymousId'] == user].to_csv('olduserData\\\\'+user+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.models.Sequential(\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None,3])\n",
    ")\n",
    "model_rnn.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "for i,user in enumerate(users):\n",
    "    t=df[df['anonymousId'] == user]\n",
    "    t=t.to_numpy()[np.newaxis,...]\n",
    "    X = np.float32(t[:,:,1:-1])\n",
    "    y = np.array([[np.float32(np.max(t[:,:,-1]))]])\n",
    "    model_rnn.fit(X,y,epochs=10,verbose=0)\n",
    "\n",
    "history = model_rnn.fit(X,y,epochs=10,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/guides/functional_api/#manipulate-complex-graph-topologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "x = np.arange(20).reshape(2, 2, 5)\n",
    "y = np.arange(20).reshape(2, 2, 5)\n",
    "keras.layers.Concatenate(axis=2)([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df=pd.read_csv('2000 users CJM.csv')\n",
    "\n",
    "categorical_columns = ['eventName', 'eventPageType', 'eventPageCategory', 'skuCode', 'productCategory',\n",
    "                       'source', 'medium', 'campaign', 'isTransaction']\n",
    "timestamp_columns = ['sentAt', 'session_start', 'previousSessionStart']\n",
    "numerical_columns = ['transactionRevenue', 'totalTransactions_current', 'totalTransactionsPerUser',\n",
    "                     'session_num', 'secondsSinceLastSession', 'secondsSinceLastEvent', 'eventNumBeforeTransaction',\n",
    "                     'eventNumInSession', 'sessionsCount']\n",
    "meta_columns = ['anonymousId', 'sessionId', 'url']\n",
    "\n",
    "target_columns = ['everTransacted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city(row):\n",
    "    res = re.findall(r'https:\\/\\/(\\w*)?\\.?petrovich\\.ru', row)\n",
    "    if len(res) != 0:\n",
    "        if res[-1] == '':\n",
    "            return 'spb'\n",
    "        else:\n",
    "            return res[-1]\n",
    "    else:\n",
    "        return 'spb'\n",
    "        \n",
    "df['city'] = df['url'].apply(extract_city)\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].fillna('(empty)').astype('str')\n",
    "    \n",
    "for col in numerical_columns:\n",
    "    df[col] = df[col].fillna(0.0).astype(np.float32)\n",
    "    \n",
    "cols = (categorical_columns + numerical_columns)\n",
    "cols.append('anonymousId')\n",
    "cols.append('city')\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### кодирование категориальных переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_encoder_for_category(column_name):\n",
    "    values = df[col].unique()\n",
    "    values = np.sort(values)\n",
    "    encoder = dict(zip(values,range(1,len(values)+1)))\n",
    "    # сохранение словаря в json-файл\n",
    "    with open('categoricalEncoders\\\\'+column_name+'.json', 'w') as f:\n",
    "        json.dump(encoder, f)\n",
    "        \n",
    "    return encoder\n",
    "\n",
    "def get_heavy_encoder(col):\n",
    "    t=pd.Series(df[col].unique()).reset_index()\n",
    "    t['index'] += 1\n",
    "    t['index'] = t['index'].astype('str')\n",
    "    tt = t.T\n",
    "    tt.columns = tt.loc[0,:]\n",
    "    tt = tt.drop(0)\n",
    "    encoder = json.loads(tt.to_json(orient='records')[1:-1])\n",
    "    # сохранение словаря в json-файл\n",
    "    with open('categoricalEncoders\\\\'+col+'.json', 'w') as f:\n",
    "        json.dump(encoder, f)\n",
    "        \n",
    "    return encoder\n",
    "\n",
    "for col in track(categorical_columns):\n",
    "    print(col)\n",
    "    # следующие столбцы имеют слишком большое количество уникальных значений, поэтому их надо сохранить отдельно\n",
    "    if col in ['skuCode', 'productCategory']:\n",
    "        df[col] = df[col].replace({'(empty)':'0.0'})\n",
    "        continue\n",
    "    cat_encoder = get_encoder_for_category(col)\n",
    "    df[col] = df[col].replace(cat_encoder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col='skuCode'\n",
    "t=pd.Series(df[col].unique()).reset_index()\n",
    "t['index'] += 1\n",
    "t['index'] = t['index'].astype('str')\n",
    "tt = t.T\n",
    "tt.columns = tt.loc[0,:]\n",
    "tt = tt.drop(0)\n",
    "skuCodeEncoder = json.loads(tt.to_json(orient='records')[1:-1])\n",
    "# сохранение словаря в json-файл\n",
    "with open('categoricalEncoders\\\\'+col+'.json', 'w') as f:\n",
    "    json.dump(skuCodeEncoder, f)\n",
    "    \n",
    "col='productCategory'\n",
    "t=pd.Series(df[col].unique()).reset_index()\n",
    "t['index'] += 1\n",
    "t['index'] = t['index'].astype('str')\n",
    "tt = t.T\n",
    "tt.columns = tt.loc[0,:]\n",
    "tt = tt.drop(0)\n",
    "productCategoryEncoder = json.loads(tt.to_json(orient='records')[1:-1])\n",
    "# сохранение словаря в json-файл\n",
    "with open('categoricalEncoders\\\\'+col+'.json', 'w') as f:\n",
    "    json.dump(productCategoryEncoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _id in track(df['anonymousId'].unique()):\n",
    "    df_to_store = df[df['anonymousId'] == _id]\n",
    "    everTransacted = df_to_store['everTransacted'].unique()\n",
    "    classLabel = df_to_store['everTransacted'].unique()[0] if len(everTransacted) == 1 else '2'\n",
    "    df_to_store = df_to_store.drop(columns=['anonymousId', 'everTransacted'])\n",
    "    df_to_store.to_csv('targetUserData\\\\'+classLabel+_id+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in track(glob('targetUserData\\\\*.csv')):\n",
    "    dft = pd.read_csv(filename)\n",
    "    dft['skuCode'] = dft['skuCode'].replace(skuCodeEncoder)\n",
    "    dft['productCategory'] = dft['productCategory'].replace(productCategoryEncoder)\n",
    "    dft.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# выбор архитектуры"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=10)\n",
    "print(vectorizer('from')) \n",
    "вернет массив, длиной output_sequence_length, \n",
    "в котором первому слову данного текста (здесь: весь текст это from) будет присовена метка, а остальным элементам будет дан 0\n",
    "фишка в том, что output_sequence_length определяет, сколько первых слов будет взято из исходного текста\n",
    "\n",
    "batch_size = 27\n",
    "tf.data.Dataset.from_tensor_slices(train_samples).batch(batch_size)\n",
    "разделяет исходный list на тензоры (список типа tf.Tensor), размером batch_size; каждый элемент тензора есть закодированный текст\n",
    "\n",
    "\n",
    "# пример использования генератора, которые на каждой итерации возвращает семпл\n",
    "x_train = glob.glob('./dataset/train/x/*.npy')\n",
    "y_train = glob.glob('./dataset/train/y/*.npy')\n",
    "x_test = glob.glob('./dataset/test/x/*.npy')\n",
    "y_test = glob.glob('./dataset/test/y/*.npy')\n",
    "def train_generator(data='train'):\n",
    "    while True:\n",
    "        if data==\"train\":\n",
    "            d = zip(x_train, y_train)\n",
    "        if data==\"test\":\n",
    "            d = zip(x_test, y_test)\n",
    "        for i,j in d:\n",
    "            train=np.load(i)\n",
    "            test = np.load(j)\n",
    "            # train = create_cutted_array(train)\n",
    "            # test  = create_cutted_array(test)\n",
    "            # for k in range(train.shape[0]):\n",
    "            a = train.reshape(1,train.shape[0],train.shape[1],train.shape[2])\n",
    "            b = test.reshape(1,test.shape[0],test.shape[1],test.shape[2])\n",
    "            yield (a, b)\n",
    "model.fit_generator(train_generator('train'), steps_per_epoch=steps_per_epoch_train, epochs=epoch,shuffle=False, verbose=1,\n",
    "                    validation_data=train_generator('test'),validation_steps=steps_per_epoch_test,callbacks=[PredictionCallback(),mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userPaths = glob('targetUserData\\\\*.csv')[:3]\n",
    "df = pd.concat([pd.read_csv(filename) for filename in userPaths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ=0\n",
    "for col in categorical_columns:\n",
    "    summ += len(df[col].unique())\n",
    "    \n",
    "summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(mode):\n",
    "    filenames = glob('targetUserData\\\\*.csv')\n",
    "    shuffle(filenames)\n",
    "    classLabels = list(map(lambda x: x.split('\\\\')[-1][0], filenames))\n",
    "    filenames_list, classLabels_list= None, None\n",
    "    if mode == 'train':\n",
    "        filenames_list, classLabels_list = filenames[:1500], classLabels[:1500]\n",
    "    elif mode == 'validation':\n",
    "        filenames_list, classLabels_list = filenames[1500:], classLabels[1500:]\n",
    "    \n",
    "    if not classLabels_list is None and not filenames_list is None:\n",
    "        for filename, classLabel in zip(filenames_list, classLabels_list):\n",
    "            features = pd.read_csv(filename)\n",
    "            n_timestamps = features.shape[0]\n",
    "            categorical_features = features[categorical_columns].to_numpy()\n",
    "            numerical_features = features[numerical_columns].to_numpy()\n",
    "            target = np.array([np.int32(classLabel)]).repeat(n_timestamps)\n",
    "#             target = np.array([np.int32(classLabel)])\n",
    "#             yield ({'categorical_features':categorical_features, 'numerical_features':numerical_features}, {'output_layer':target})\n",
    "            yield tuple([(categorical_features, numerical_features), target])\n",
    "\n",
    "gen_train = data_generator('train')\n",
    "gen_valid = data_generator('validation')\n",
    "\n",
    "tt1 = next(gen_train)\n",
    "tt2 = next(gen_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_generator(data_generator, args=['train'], output_types=(tf.int32))\n",
    "dataset_valid = tf.data.Dataset.from_generator(data_generator, args=['validation'], output_types=(tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_generator(filenames, mode):\n",
    "    filenames_list = None\n",
    "    if mode == 'train':\n",
    "        filenames_list = filenames[:1500]\n",
    "    elif mode == 'validation':\n",
    "        filenames_list = filenames[1500:]\n",
    "    \n",
    "    if not filenames_list is None:\n",
    "        for filename in filenames_list:\n",
    "            features = pd.read_csv(filename)\n",
    "            n_timestamps = features.shape[0]\n",
    "            categorical_features = features[categorical_columns].to_numpy()\n",
    "            numerical_features = features[numerical_columns].to_numpy()\n",
    "            yield {'categorical_features':categorical_features, 'numerical_features':numerical_features}\n",
    "            \n",
    "def y_generator(filenames, mode):\n",
    "    classLabels = list(map(lambda x: x.split('\\\\')[-1][0], filenames))\n",
    "    filenames_list, classLabels_list = None, None\n",
    "    if mode == 'train':\n",
    "        filenames_list, classLabels_list = filenames[:1500], classLabels[:1500]\n",
    "    elif mode == 'validation':\n",
    "        filenames_list, classLabels_list = filenames[1500:], classLabels[1500:]\n",
    "    \n",
    "    if not classLabels_list is None and not filenames_list is None:\n",
    "        for filename, classLabel in zip(filenames_list, classLabels_list):\n",
    "            features = pd.read_csv(filename)\n",
    "            n_timestamps = features.shape[0]\n",
    "            target = np.array([np.int32(classLabel)]).repeat(n_timestamps)\n",
    "            yield {'output_layer':target}\n",
    "            \n",
    "filenames = glob('targetUserData\\\\*.csv')\n",
    "shuffle(filenames)\n",
    "x_generator_train = x_generator(filenames, 'train')\n",
    "y_generator_train = y_generator(filenames, 'train')\n",
    "x_generator_valid = x_generator(filenames, 'validation')\n",
    "y_generator_valid = y_generator(filenames, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 25000\n",
    "output_vector_shape = 8\n",
    "categorical_columns_len = len(categorical_columns)\n",
    "numerical_columns_len = len(numerical_columns)\n",
    "cat_input = keras.Input(\n",
    "    shape=(None,categorical_columns_len,), name='categorical_features'\n",
    ")\n",
    "num_input = keras.Input(\n",
    "    shape=(None,numerical_columns_len), name='numerical_features'\n",
    ")\n",
    "\n",
    "cat_features = keras.layers.Embedding(vocab_size, output_vector_shape, name='embedded_categorical_features')(cat_input)\n",
    "unstacked = keras.layers.Lambda(lambda x: tf.unstack(x, axis=2), name='unstack_layer')(cat_features)\n",
    "dense_outputs = [keras.layers.Dense(1, name='embedded_feature_denser_'+str(i))(l) for i,l in enumerate(unstacked)]\n",
    "merged = keras.layers.Lambda(lambda x: tf.stack(x, axis=2), name='stack_layer')(dense_outputs)\n",
    "squeezed = keras.layers.Lambda(lambda x: tf.squeeze(x, axis=3), 'squeeze_layer')(merged)\n",
    "\n",
    "concated = keras.layers.Concatenate(name='concatenation_layer')([num_input, squeezed])\n",
    "rnn_layer = keras.layers.SimpleRNN(64, input_shape=[None,1], name='RNN_layer')(concated)\n",
    "output_layer = keras.layers.Dense(1, name='output_layer')(rnn_layer)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[cat_input, num_input],\n",
    "    outputs=[output_layer]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss = {\n",
    "        'output_layer': keras.losses.BinaryCrossentropy()\n",
    "    }\n",
    ")\n",
    "\n",
    "# опреатор yield генератора должен возвращать следующую структуру:\n",
    "# yield ({'categorical_features':categorical_features, 'numerical_features':numerical_features},{'output_layer':output_layer})\n",
    "model.fit(\n",
    "    data_generator('train'),\n",
    "    validation_data = data_generator('validation'),\n",
    "    epochs=1,\n",
    "#     batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(filename) for filename in glob('targetUserData\\\\*.csv')]).fillna(0.0).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "t = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "def input_solver(sample):\n",
    "    s1 = sample[categorical_columns]\n",
    "    s2 = sample[numerical_columns]\n",
    "    s3 = sample['totalTransactionPerUser']\n",
    "    return {'categorical_features':s1, \n",
    "            'numerical_featurs':s2}, {'output_layer':s3}\n",
    "    \n",
    "t.map(input_solver)\n",
    "model.fit(t, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# запуск обучения на массивах numpy\n",
    "1. Массив np, хранящий категориальные признаки: X_train_cat, shape=(N_users, n_timestamps, n_features)\n",
    "2. Массив np, хранящий количественные признаки: X_train_num, shape=(N_users, n_timestamps, n_features)\n",
    "3. Массив np, хранящий значения класса: y_train_num, shape=(N_users, 1)\n",
    "4. Повторить предыдущие шаги для образования тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('targetUserData\\\\*.csv')\n",
    "shuffle(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_label_from_filename = lambda fname: np.int8(fname.split('\\\\')[-1][0])\n",
    "get_cat_data_from_dataframe = lambda df: df[categorical_columns].to_numpy()\n",
    "get_num_data_from_dataframe = lambda df: df[numerical_columns].to_numpy()\n",
    "\n",
    "def read_singleUserData(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    classLabel = get_label_from_filename(filename)\n",
    "    # массивы NumPy\n",
    "    cat_data = get_cat_data_from_dataframe(df)\n",
    "    num_data = get_num_data_from_dataframe(df)\n",
    "    return cat_data, num_data, classLabel\n",
    "\n",
    "def read_userData(filenames):\n",
    "    df_cat, df_num, df_target = [], [], []\n",
    "    for filename in filenames:\n",
    "        cat_data, num_data, classLabel = read_singleUserData(filename)\n",
    "        df_cat.append(cat_data)\n",
    "        df_num.append(num_data)\n",
    "        df_target.append(classLabel)\n",
    "    return df_cat, df_num, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data, num_data, target = read_userData(filenames[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "#     x={'categorical_features':cat_data[0], 'numerical_features':num_data[0]},\n",
    "#     y={'output_layer':target},\n",
    "    x=(cat_data[0], num_data[0]),\n",
    "    y=np.array(target).repeat(24),\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
